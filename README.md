# DEEPSPEED-FINETUNING-TURIAL
[**ğŸŒEnglish**](./README-en.md) âˆ™ [**ğŸ“–ç®€ä½“ä¸­æ–‡**](./README.md)

è¿™ä¸ªé¡¹ç›®æ˜¯ä¸€ä¸ªå‹å¥½çš„ã€é›¶åŸºç¡€è®­ç»ƒæ¨¡æ¿æ•™ç¨‹ï¼Œä»£ç ä¸­åŒ…å«å¤§é‡æ³¨é‡Šï¼Œè¯¦ç»†ä»‹ç»å¦‚ä½•ä½¿ç”¨deepspeedè¿›è¡Œfinetuningï¼Œä»¥åŠå¦‚ä½•ä½¿ç”¨deepspeedè¿›è¡Œåˆ†å¸ƒå¼è®­ç»ƒï¼Œä»‹ç»å¦‚ä½•ä½¿ç”¨deepspeedè¿›è¡Œæ¨¡å‹è®­ç»ƒã€‚å¯ä»¥å¿«é€Ÿå®ç°å¤šç§ä»»åŠ¡çš„å¾®è°ƒå’Œé’ˆå¯¹ä¸åŒçš„æ¨¡å‹ï¼Œæ ¹æ®éœ€æ±‚è¿›è¡Œä¿®æ”¹argparseå‚æ•°æˆ–è€…æºä»£ç å³å¯ã€‚è¯¥é¡¹ç›®ä»¥æ–‡æœ¬æ€»ç»“æ•°æ®é›†LCSTS: A Large-Scale Chinese Short Text Summarization Datasetä¸ºåŸºç¡€ï¼Œå¾®è°ƒä¸€ä¸ªé’ˆå¯¹ä¸­æ–‡æ–‡æœ¬æ€»ç»“çš„å¤§æ¨¡å‹ã€‚

**æ–‡ä»¶ç›®å½•å¦‚ä¸‹ï¼š**

```shell
.
â”œâ”€â”€ è¯»æˆ‘.md
â”œâ”€â”€ checkpoints
â”œâ”€â”€ config
â”‚   â””â”€â”€ dsconfig.json
â”œâ”€â”€ data
â”‚   â”œâ”€â”€ test.jsonl
â”‚   â”œâ”€â”€ test_public.jsonl
â”‚   â”œâ”€â”€ train.jsonl
â”‚   â””â”€â”€ valid.jsonl
â”œâ”€â”€ fig
â”‚   â””â”€â”€ estimate_memory.png
â”œâ”€â”€ logs
â”œâ”€â”€ readme.md
â”œâ”€â”€ shell
â”‚   â”œâ”€â”€ run_multi_gpu_freeze.sh
â”‚   â”œâ”€â”€ run_multi_gpu_full-tuning.sh
â”‚   â”œâ”€â”€ run_multi_gpu_lora.sh
â”‚   â”œâ”€â”€ run_single_gpu_freeze.sh
â”‚   â”œâ”€â”€ run_single_gpu_full-tuning.sh
â”‚   â””â”€â”€ run_single_gpu_lora.sh
â”œâ”€â”€ train.py
â””â”€â”€ util
    â”œâ”€â”€ dataprocess.py
    â”œâ”€â”€ __init__.py
    â”œâ”€â”€ __pycache__
    â”‚   â”œâ”€â”€ dataprocess.cpython-311.pyc
    â”‚   â”œâ”€â”€ __init__.cpython-311.pyc
    â”‚   â””â”€â”€ tools.cpython-311.pyc
    â””â”€â”€ tools.py
```

# è®­ç»ƒæµç¨‹æ­¥éª¤æ¨¡æ¿

## é¢„ä¼°æ‰€éœ€çš„å†…å­˜

ä½¿ç”¨estimate_zero3_model_states_mem_needs_all_liveå‡½æ•°ï¼Œä¼ å…¥æ¨¡å‹ï¼Œä»¥åŠgpuæ•°é‡å’ŒèŠ‚ç‚¹æ•°é‡ï¼Œè¿”å›ä¸€ä¸ªå†…å­˜å¤§å°ï¼Œè¿™ä¸ªå†…å­˜å¤§å°åŒ…æ‹¬ä¸åŒç»„åˆçš„å†…å­˜å¤§å°ï¼Œè¾…åŠ©æˆ‘ä»¬è¯„ä¼°æ‰€éœ€è¦çš„å†…å­˜å’Œæ˜¾å­˜ã€‚
```python
from transformers import AutoModel,AutoTokenizer; \
from deepspeed.runtime.zero.stage3 import estimate_zero3_model_states_mem_needs_all_live; \
model = AutoModel.from_pretrained("THUDM/chatglm3-6b", trust_remote_code=True); \
estimate_zero3_model_states_mem_needs_all_live(model, num_gpus_per_node=6, num_nodes=1)
```

![estimate_memory](/fig/estimate_memory.png)

## å¤„ç†æ•°æ®é›†
è¿™ä¸€æ­¥åŒ…æ‹¬å®šä¹‰ä¸€ä¸ªæ•°æ®é›†ç±»CustomDatasetï¼Œç»§æ‰¿torch.utils.data.Datasetï¼Œé‡å†™`__getitem__`å’Œ`__len__`æ–¹æ³•ï¼Œè¿”å›ä¸€ä¸ªbatchçš„æ•°æ®ï¼Œè¿™é‡Œå¯ä»¥å‚è€ƒpytorchå®˜æ–¹æ–‡æ¡£ï¼Œæˆ–è€…å‚è€ƒæˆ‘å†™çš„ä¸€ä¸ªä¾‹å­ã€‚

## å®šä¹‰æ¨¡å‹
æ¨¡å‹å¯ä»¥æ˜¯pytorchå®šä¹‰çš„ï¼Œä¹Ÿå¯ä»¥æ˜¯transformerså®šä¹‰çš„ï¼Œè¿™é‡Œæƒ³è¦å¾®è°ƒçš„æ˜¯chatglmæ¨¡å‹ï¼Œè‹¥æ˜¯ä½ æƒ³è¦è®­ç»ƒå…¶ä»–æ¨¡å‹å¦‚gptï¼Œå¯ä»¥ä¿®æ”¹`--model_name_or_path`å‚æ•°ã€‚æœ¬é¡¹ç›®ä½¿ç”¨çš„æ¨¡å‹æ˜¯chatglmï¼Œå› æ­¤è·¯å¾„æ˜¯`--model_name_or_path THUDM/chatglm-6b`ã€‚å¦‚æœä½ æ˜¯è‡ªå·±å®šä¹‰çš„æ¨¡å‹å¯ä»¥å°†æºä»£ç ä¸­çš„`model=...`ä¿®æ”¹ä¸ºpytorchä»£ç å®šä¹‰çš„æ¨¡å‹å³å¯ã€‚

## è®­ç»ƒæ¨¡å‹ä»£ç train.py

### Argument Parsing

å®šä¹‰`parse_args`å‡½æ•°ï¼š

```python
def parse_args():
    parser = argparse.ArgumentParser()
    # è¿™é‡Œå¯ä»¥å…ˆç”¨parser.add_argumentæ·»åŠ ä¸€äº›å…¶ä»–å‚æ•°ï¼Œä¾‹å¦‚å­¦ä¹ ç‡ï¼Œepochï¼Œbatch_sizeç­‰å‚æ•°
    ...
    
    # è¿™é‡Œå¯ä»¥é€šè¿‡jsonæ¨¡å—å¯¼å…¥configæ–‡ä»¶ç»™ä¸€ä¸ªå˜é‡ï¼Œå¹¶é€šè¿‡è¯¥å˜é‡ä¼ ç»™deepspeed.initializeçš„configå‚æ•°ï¼›ä¹Ÿå¯ä»¥åœ¨shellå‘½ä»¤è¡Œé‡Œé¢é€šè¿‡deepspeed --deepspeed --deepspeed_config deepspeed_config.jsonçš„æ–¹å¼ï¼Œè¯¥æ–¹å¼å°±ä¸éœ€è¦é€šè¿‡è¯¥å˜é‡ä¼ ç»™deepspeed.initializeçš„configå‚æ•°
    parser.add_argument("--ds_config", type=str, default="deepspeed_config.json", help="deepspeed_configè·¯å¾„")
    
    # å®šä¹‰ä¸€äº›deepspeedä¼šè‡ªåŠ¨æ”¹å˜çš„å˜é‡ï¼Œlocal_rankï¼Œgloba_lrankï¼Œlocal_rankä¸ºgpuç¼–å·ï¼ˆå•ä¸ªèŠ‚ç‚¹å†…çš„ï¼‰ï¼Œgloba_lrankä¸ºgpuå…¨å±€ç¼–å·ï¼ˆå¤šä¸ªèŠ‚ç‚¹ï¼‰
    parser.add_argument("--local_rank", type=int, default=-1, help="local_rank")
    
    # åŠ å…¥deepspeedçš„arguments
    parser = deepspeed.add_config_arguments(parser)
    return parser.parse_args()
```

### deepspeed.init_distributed

éœ€è¦åœ¨åˆå§‹åŒ–deepspeed.initializeå‰ä½¿ç”¨ï¼Œä½œç”¨å’Œtorch.distributed.init_process_group(...)ç±»ä¼¼

å…¶å®deepspeedçš„mainä»£ç å’Œtorch.distributedæœ‰ç‚¹åƒ

```python
args = parse_args()

if args.local_rank == -1:
    device = torch.device("cuda")
else:
    torch.cuda.set_device(args.local_rank)
    device = torch.device("cuda", args.local_rank)
    deepspeed.init_distributed()
args.global_rank = torch.distributed.get_rank()
```

### åˆå§‹åŒ–deepspeed.initialize

`deepspeed.initialize`ä½¿ç”¨æ–¹æ³•ï¼š

- è¿”å›å‚æ•°æœ‰ï¼š[engine, engine.optimizer, engine.training_dataloader, engine.lr_scheduler]

- ä¸€èˆ¬è¾“å…¥å‚æ•°:

  -  model: å¿…é¡»ï¼Œä¼ å…¥çš„model

  -  optimizer: å¯é€‰: torchå®šä¹‰æˆ–è€…config.jsonå®šä¹‰

  -  model_parameters: å¯é€‰ï¼šæ¨¡å‹çš„parameters

  -  training_data: å¯é€‰ï¼Œdatasetç±»å‹

  -  lr_schduler: å¯é€‰: torchå®šä¹‰æˆ–è€…config.jsonå®šä¹‰

  -  config: å¯é€‰ï¼šconfigæ–‡ä»¶å†…å®¹ï¼Œ å½“ä½ ä¸æƒ³ä¼ å…¥configå‚æ•°ï¼Œå¯ä»¥é€šè¿‡åœ¨shellå‘½ä»¤è¡Œé‡Œé¢é€šè¿‡`deepspeed --deepspeed --deepspeed_config deepspeed_config.json`çš„æ–¹å¼ï¼Œè¯¥æ–¹å¼å°±ä¸éœ€è¦é€šè¿‡è¯¥å˜é‡ä¼ ç»™deepspeed.initializeçš„configå‚æ•°ï¼Œè¿™é‡Œé‡‡ç”¨çš„æ˜¯ä¼ å…¥å‚æ•°configï¼Œå› ä¸ºå¯ä»¥æ›´åŠ çµæ´»çš„æ”¹å˜å‚æ•°ï¼Œæ¯”å¦‚gradient_accumulation_stepsç­‰

- ç”¨æ³•ä¸¾ä¾‹`model, optimizer, _, lr_scheduler = deepspeed.initialize(model=model, args=args, config=ds_config)`

### forå¾ªç¯è®­ç»ƒ

```python
model_engine.train()
for step, batch in enumerate(data_loader):
    #forward() method
    loss = model_engine(batch)

    #runs backpropagation
    model_engine.backward(loss)

    #weight update
    model_engine.step()
```

è¿™é‡Œä¼šè‡ªåŠ¨è°ƒç”¨schedulerå’Œoptimizerï¼Œå¦‚æœæŒ‡å®šgradient_accumulation_stepsï¼Œä¼šåœ¨gradient_accumulation_stepsæ‰è¿›è¡Œä¸€æ¬¡updateï¼Œè¿™é‡Œmodel_engineçš„è¾“å…¥è¾“å‡ºå‚æ•°å’ŒåŸæ¥çš„modelæ˜¯å®Œå…¨ä¸€æ ·çš„ã€‚åœ¨è·å¾—lossåï¼Œä½ å¯ä»¥åªæ˜¯åœ¨è°ƒç”¨model_engine.backward(loss)å’Œmodel_engine.step()ï¼Œæ— éœ€è°ƒç”¨model.zero_grad()å’Œoptimizer.step()ç­‰æ“ä½œã€‚

## å®šä¹‰deepspeedçš„configçš„jsonæ–‡ä»¶

æ¯ä¸ªå‚æ•°æ„ä¹‰çœ‹è¿™ä¸ªæ–‡æ¡£https://www.deepspeed.ai/docs/config-json/#batch-size-related-parametersï¼Œå¯æ ¹æ®éœ€æ±‚è¿›è¡Œå®šä¹‰
ZeRO3åˆ†å¸ƒå¼è®­ç»ƒconfigçš„jsonæ–‡ä»¶å®šä¹‰æ¨¡æ¿ï¼š

```json
{
   //è¿™é‡Œå®šä¹‰train_micro_batch_size_per_gpuå’Œgradient_accumulation_stepsç„¶åå¿½ç•¥train_batch_sizeï¼Œå› ä¸ºtrain_batch_size = train_micro_batch_size_per_gpu * gpuæ•°é‡ * gradient_accumulation_steps
  "gradient_accumulation_steps": 32,
  "train_micro_batch_size_per_gpu": 4,
    //å¤šå°‘ä¸ªstepæ‰“å°ä¸€æ¬¡ä¿¡æ¯ï¼Œé»˜è®¤10
  "steps_per_print": 1,
    //ZeROä¼˜åŒ–ï¼Œè¿™é‡Œé€‰æ‹©çš„æ˜¯é˜¶æ®µ3ï¼Œå¸è½½åˆ°cpuå†…å­˜ä¸Š
  "zero_optimization": {
    "stage": 3,
    "offload_param": {
      "device": "cpu"
    },
    "offload_optimizer": {
      "device": "cpu"
    },
    "stage3_gather_16bit_weights_on_model_save": true
  },
  "bf16": {
    "enabled": false
  },
  "fp16": {
    "enabled": true,
    "loss_scale": 0,
    "loss_scale_window": 100
  },
    //æ¢¯åº¦å¤§äº1è¿›è¡Œè£å‰ª
  "gradient_clipping": 1.0,
	//ä¸‹é¢ä¼˜åŒ–å™¨å’Œå­¦ä¹ ç‡ä¼˜åŒ–å™¨å¯ä»¥è‡ªå·±pytorchå®šä¹‰ä¼ å…¥deepspeed.initialize
  "scheduler": {
        "type": "WarmupDecayLR",
        "params": {
            ...
        }
    },
  "optimizer": {
        "type": "AdamW",
        "params": {
            ...
        }
    },
    //è¿™ä¸ªå‚æ•°è®©ä½ èƒ½å¤Ÿä½¿ç”¨è‡ªå·±å®šä¹‰çš„torch.optimï¼Œä½†æ˜¯å¦‚æœä½ æƒ³ç”¨å®˜æ–¹çš„ä½ å°±æŠŠä¸‹é¢è¿™å¥å»æ‰å³å¯
   "zero_force_ds_cpu_optimizer": false
}
```

è¿™é‡Œæ˜¯ä¸ºäº†èƒ½å¤Ÿè·å¾—è‡ªå·±æƒ³è¦çš„å­¦ä¹ ç‡æ›²çº¿ï¼Œæ‰€ä»¥é‡‡ç”¨è‡ªå®šä¹‰çš„optimizerå’Œschedulerï¼Œä½†æ˜¯å®˜æ–¹æ¨èç”¨ä»–ä»¬å®šä¹‰çš„optimizerå’Œschedulerï¼š

```
[rank0]: deepspeed.runtime.zero.utils.ZeRORuntimeException: You are using ZeRO-Offload with a client provided optimizer (<class 'torch.optim.adamw.AdamW'>) which in most cases will yield poor performance. Please either use deepspeed.ops.adam.DeepSpeedCPUAdam or set an optimizer in your ds-config (https://www.deepspeed.ai/docs/config-json/#optimizer-parameters). If you really want to use a custom optimizer w. ZeRO-Offload and understand the performance impacts you can also set <"zero_force_ds_cpu_optimizer": false> in your configuration file.
```

## è¿è¡ŒshellæŒ‡ä»¤

è¿™é‡Œåˆ†ä¸ºå•gpuå’Œå¤šgpuè®­ç»ƒï¼Œå…·ä½“å‚æ•°æ„æ€å¯ä»¥çœ‹train.pyçš„parse_argså‡½æ•°ï¼Œé‡Œé¢éƒ½æœ‰è¯¦ç»†æ³¨é‡Šã€‚
### å•gpuè®­ç»ƒ

å®˜æ–¹æ¨èä½¿ç”¨--include localhost:...å‚æ•°ï¼Œå»æŒ‡å®šgpuï¼Œè€Œä¸æ˜¯é€šè¿‡CUDA_VISIBLE_DEVICES=...ï¼Œè€Œä¸”å¯ä»¥é€šè¿‡`--master_port=<port>`æŒ‡å®šè¿è¡Œçš„ç«¯å£ã€‚ä¸åŠ çš„è¯ï¼Œå°±æ˜¯ç³»ç»Ÿé»˜è®¤åˆ†é…ã€‚å…³äº`--offload_device nvme`æ˜¯é€‰nvmeè¿˜æ˜¯cpuï¼Œå¦‚æœcpuå†…å­˜ä¸å¤Ÿå°±é€‰nvmeã€‚

#### LoRAå¾®è°ƒ

è¿è¡Œ`sh ./shell/run_single_gpu_lora.sh`

```shell
#!/bin/bash

deepspeed --include localhost:0 --master_port 12345 train.py \
          --model_name_or_path THUDM/chatglm3-6b \
          --src_len 512 \
          --tgt_len 128 \
          --data_path ./data/test.jsonl \
          --train_micro_batch_size_per_gpu 1\
          --gradient_accumulation_steps 16 \
          --max_lr 1e-4 \
          --initial_lr 1e-6 \
          --min_lr 1e-8 \
          --weight_decay 0.01 \
          --adam_beta1 0.9 \
          --adam_beta2 0.999\
          --epochs 1 \
          --output_dir ./checkpoints/ \
          --finetune_method lora \
          --ds_config_path ./config/dsconfig.json \
          --lora_alpha 32 \
          --lora_dropout 0.05 \
          --lora_target_modules query_key_value \
          --lora_r 8 \
          --offload_device cpu \
          --nvme_path ./mnt/nvme
```
#### Freeze å†»ç»“éƒ¨åˆ†å‚æ•°å¾®è°ƒ

è¿è¡Œ`sh ./shell/run_single_gpu_freeze.sh`

```shell
#!/bin/bash

deepspeed --include localhost:0 --master_port 12345 train.py \
          --model_name_or_path THUDM/chatglm3-6b \
          --src_len 512 \
          --tgt_len 128 \
          --data_path ./data/test.jsonl \
          --train_micro_batch_size_per_gpu 1\
          --gradient_accumulation_steps 16 \
          --max_lr 1e-4 \
          --initial_lr 1e-6 \
          --min_lr 1e-8 \
          --weight_decay 0.01 \
          --adam_beta1 0.9 \
          --adam_beta2 0.999\
          --epochs 1 \
          --output_dir ./checkpoints \
          --finetune_method freeze \
          --freeze_modules dense_h_to_4h \
          --ds_config_path ./config/dsconfig.json \
          --offload_device cpu \
          --nvme_path ./mnt/nvme
```

#### å…¨é‡å¾®è°ƒ

è¿è¡Œ`sh ./shell/run_single_gpu_full-tuning.sh`

```shell
#!/bin/bash

deepspeed --include localhost:0 --master_port 12345 train.py \
          --model_name_or_path THUDM/chatglm3-6b \
          --src_len 512 \
          --tgt_len 128 \
          --data_path ./data/test.jsonl \
          --train_micro_batch_size_per_gpu 1\
          --gradient_accumulation_steps 16 \
          --max_lr 1e-4 \
          --initial_lr 1e-6 \
          --min_lr 1e-8 \
          --weight_decay 0.01 \
          --adam_beta1 0.9 \
          --adam_beta2 0.999\
          --epochs 1 \
          --output_dir ./checkpoints \
          --finetune_method full-tuning \
          --ds_config_path ./config/dsconfig.json \
          --offload_device cpu \
          --nvme_path ./mnt/nvme
```

### å¤šgpuè®­ç»ƒ

#### LoRAå¾®è°ƒ

è¿è¡Œ`sh ./shell/run_multi_gpu_lora.sh`

```shell
#!/bin/bash

deepspeed --include localhost:0,1,2,3,4,5,6 --master_port 12345 train.py \
          --model_name_or_path THUDM/chatglm3-6b \
          --src_len 512 \
          --tgt_len 128 \
          --data_path ./data/test.jsonl \
          --train_micro_batch_size_per_gpu 1\
          --gradient_accumulation_steps 16 \
          --max_lr 1e-4 \
          --initial_lr 1e-6 \
          --min_lr 1e-8 \
          --weight_decay 0.01 \
          --adam_beta1 0.9 \
          --adam_beta2 0.999\
          --epochs 1 \
          --output_dir ./checkpoints/ \
          --finetune_method lora \
          --ds_config_path ./config/dsconfig.json \
          --lora_alpha 32 \
          --lora_dropout 0.05 \
          --lora_target_modules query_key_value \
          --lora_r 8 \
          --offload_device cpu \
          --nvme_path ./mnt/nvme
```

#### Freeze å†»ç»“éƒ¨åˆ†å‚æ•°å¾®è°ƒ

è¿è¡Œ`sh ./shell/run_multi_gpu_freeze.sh`

```shell
#!/bin/bash

deepspeed --include localhost:0,1,2,3,4,5,6 --master_port 12345 train.py \
          --model_name_or_path THUDM/chatglm3-6b \
          --src_len 512 \
          --tgt_len 128 \
          --data_path ./data/test.jsonl \
          --train_micro_batch_size_per_gpu 1\
          --gradient_accumulation_steps 16 \
          --max_lr 1e-4 \
          --initial_lr 1e-6 \
          --min_lr 1e-8 \
          --weight_decay 0.01 \
          --adam_beta1 0.9 \
          --adam_beta2 0.999\
          --epochs 1 \
          --output_dir ./checkpoints \
          --finetune_method freeze \
          --freeze_modules dense_h_to_4h \
          --ds_config_path ./config/dsconfig.json \
          --offload_device cpu \
          --nvme_path ./mnt/nvme
```

#### å…¨é‡å¾®è°ƒ

è¿è¡Œ`sh ./shell/run_multi_gpu_full-tuning.sh`

```shell
#!/bin/bash

deepspeed --include localhost:0,1,2,3,4,5,6 --master_port 12345 train.py \
          --model_name_or_path THUDM/chatglm3-6b \
          --src_len 512 \
          --tgt_len 128 \
          --data_path ./data/test.jsonl \
          --train_micro_batch_size_per_gpu 1\
          --gradient_accumulation_steps 16 \
          --max_lr 1e-4 \
          --initial_lr 1e-6 \
          --min_lr 1e-8 \
          --weight_decay 0.01 \
          --adam_beta1 0.9 \
          --adam_beta2 0.999\
          --epochs 1 \
          --output_dir ./checkpoints \
          --finetune_method full-tuning \
          --ds_config_path ./config/dsconfig.json \
          --offload_device cpu \
          --nvme_path ./mnt/nvme
```

# ä½ å¯èƒ½é‡è§çš„é—®é¢˜

1. RuntimeError: Unable to JIT load the async_io op due to it not being compatible due to hardware/software issue. None

å‡ºç°`RuntimeError: Unable to JIT load the async_io op due to it not being compatible due to hardware/software issue. None`é”™è¯¯æ—¶ï¼Œæ„å‘³ç€DeepSpeedçš„`async_io`æ“ä½œæ— æ³•åœ¨å½“å‰ç³»ç»Ÿä¸Šå³æ—¶ç¼–è¯‘ï¼ˆJITï¼‰åŠ è½½ï¼Œå› ä¸ºå­˜åœ¨ç¡¬ä»¶æˆ–è½¯ä»¶å…¼å®¹æ€§é—®é¢˜ã€‚è¿™ä¸ªé—®é¢˜ä¸`libaio`åº“çš„å…¼å®¹æ€§æœ‰å…³ã€‚è¦è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œä½ éœ€è¦ç¡®ä¿ç³»ç»Ÿä¸Šå®‰è£…äº†`libaio-dev`åº“ã€‚è¿™ä¸ªåº“æ˜¯`async_io`æ“ä½œæ‰€ä¾èµ–çš„ã€‚åœ¨Ubuntuç³»ç»Ÿä¸Šï¼Œä½ å¯ä»¥é€šè¿‡ä»¥ä¸‹å‘½ä»¤å®‰è£…`libaio-dev`ï¼š

```bash
sudo apt install libaio-dev
```

2. AttributeError: 'DeepSpeedCPUAdam' object has no attribute 'ds_opt_adam'

åœ¨shellå‘½ä»¤å‰è®¾ç½®å¥½ç¯å¢ƒå˜é‡export DS_SKIP_CUDA_CHECK=1ã€‚è§£å†³æ–¹æ³•æ¥è‡ªåšä¸»[deepspeedä½¿ç”¨zero3 + offloadæŠ¥é”™:AttributeError: â€˜DeepSpeedCPUAdamâ€˜ object has no attribute â€˜ds_opt_adam_deepspeed zero3 offload-CSDNåšå®¢](https://blog.csdn.net/qq_44193969/article/details/137051032)

